{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import warnings filter - ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listing books to choose from\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning, Processing & Language Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "parents = gutenberg.raw('edgeworth-parents.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "#brown\n",
    "#parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub('\\'','', text)\n",
    "    text = re.sub('`','', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406629\n",
      "935158\n"
     ]
    }
   ],
   "source": [
    "print(len(brown))\n",
    "print(len(parents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutting down to about first 20k words & cleaning \n",
    "brown = text_cleaner(brown[:20000])\n",
    "parents = text_cleaner(parents[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501536334306345\n",
      "0.49846366569365497\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "#brown\n",
    "#parents\n",
    "\n",
    "#print(len(brown))\n",
    "#print(len(parents))\n",
    "\n",
    "print(len(brown)/(len(brown)+len(parents)))\n",
    "print(len(parents)/(len(brown)+len(parents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels.\n",
    "nlp = spacy.load('en')\n",
    "brown_doc = nlp(brown)\n",
    "parents_doc = nlp(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(brown_sents + parents_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "parentswords = bag_of_words(parents_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(brownwords + parentswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marys</th>\n",
       "      <th>ballad</th>\n",
       "      <th>pasture</th>\n",
       "      <th>town</th>\n",
       "      <th>regard</th>\n",
       "      <th>activity</th>\n",
       "      <th>dim</th>\n",
       "      <th>charge</th>\n",
       "      <th>Anne</th>\n",
       "      <th>hinder</th>\n",
       "      <th>...</th>\n",
       "      <th>produce</th>\n",
       "      <th>resolve</th>\n",
       "      <th>migration</th>\n",
       "      <th>detective</th>\n",
       "      <th>good</th>\n",
       "      <th>people</th>\n",
       "      <th>straggli</th>\n",
       "      <th>fond</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I.)</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, Absence, of, Mr, Glass)</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(THE, consulting, -, rooms, of, Dr, Orion, Hoo...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, such, a, place, the, sea, had, something,...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(It, must, not, be, supposed, that, Dr, Hoods,...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Marys ballad pasture town regard activity dim charge Anne hinder  ...  \\\n",
       "0     0      0       0    0      0        0   0      0    0      0  ...   \n",
       "1     0      0       0    0      0        0   0      0    0      0  ...   \n",
       "2     0      0       0    0      0        0   0      0    0      0  ...   \n",
       "3     0      0       0    0      0        0   0      0    0      0  ...   \n",
       "4     0      0       0    0      0        0   0      0    0      0  ...   \n",
       "\n",
       "  produce resolve migration detective good people straggli fond  \\\n",
       "0       0       0         0         0    0      0        0    0   \n",
       "1       0       0         0         0    0      0        0    0   \n",
       "2       0       0         0         0    0      0        0    0   \n",
       "3       0       0         0         0    0      0        0    0   \n",
       "4       0       0         0         0    0      0        0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0                                               (I.)  Chesterton  \n",
       "1                      (The, Absence, of, Mr, Glass)  Chesterton  \n",
       "2  (THE, consulting, -, rooms, of, Dr, Orion, Hoo...  Chesterton  \n",
       "3  (In, such, a, place, the, sea, had, something,...  Chesterton  \n",
       "4  (It, must, not, be, supposed, that, Dr, Hoods,...  Chesterton  \n",
       "\n",
       "[5 rows x 1313 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = word_counts.drop(columns=['text_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "brown=gutenberg.sents('chesterton-brown.txt')\n",
    "parents=gutenberg.sents('edgeworth-parents.txt')\n",
    "\n",
    "#processing\n",
    "brown_sentences=[]\n",
    "for sentence in brown:\n",
    "    sent = sentence[0]\n",
    "    sent = [re.sub(r'--',' ',word) for word in sent]\n",
    "    sent = [re.sub(\"[\\[].*?[\\]]\", \"\", word) for word in sent]\n",
    "    sent = [re.sub('\\'','', word) for word in sent]\n",
    "    sent = [re.sub('\"','', word) for word in sent]\n",
    "    sent = [re.sub('`','', word) for word in sent]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    brown_sentences.append(''.join(sent))\n",
    "\n",
    "#processing\n",
    "parents_sentences=[]\n",
    "for sentence in parents:\n",
    "    sent=sentence[0]\n",
    "    sent = [re.sub(r'--',' ',word) for word in sent]\n",
    "    sent = [re.sub(\"[\\[].*?[\\]]\", \"\", word) for word in sent]\n",
    "    sent = [re.sub('\\'','', word) for word in sent]\n",
    "    sent = [re.sub('\"','', word) for word in sent]\n",
    "    sent = [re.sub('`','', word) for word in sent]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    parents_sentences.append(''.join(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "#print(brown_sentences[0:5])\n",
    "#print(parents_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=2, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tfidf = vectorizer.fit_transform(brown_sentences)\n",
    "\n",
    "brown_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "dense = brown_tfidf.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "df_brown = pd.DataFrame(denselist, columns=brown_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_tfidf = vectorizer.fit_transform(parents_sentences)\n",
    "\n",
    "parents_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "dense = parents_tfidf.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "df_parents = pd.DataFrame(denselist, columns=parents_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3806, 68)\n",
      "(10230, 236)\n"
     ]
    }
   ],
   "source": [
    "print(df_brown.shape)\n",
    "print(df_parents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding author column \n",
    "df_brown['text_source'] = \"Chesterton\"\n",
    "df_parents['text_source'] = \"Edgeworth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.concat([df_brown, df_parents], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14036, 280)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = df_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "\n",
    "#df_tfidf['text_source']\n",
    "#df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Supervised Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = word_counts.drop(columns=['text_source'])\n",
    "y_bow = word_counts['text_source']\n",
    "\n",
    "x_tfidf = df_tfidf.drop(columns=['text_source'])\n",
    "y_tfidf = df_tfidf['text_source']\n",
    "\n",
    "X_bow_train, X_bow_test, y_bow_train, y_bow_test = train_test_split(x_bow, y_bow, test_size=0.2, random_state=0)\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(x_tfidf, y_tfidf, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9555555555555556\n",
      "\n",
      "Test set score: 0.8088235294117647\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(random_state=1)\n",
    "\n",
    "#bow\n",
    "bow_train = rfc.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_bow_train, y_bow_train))\n",
    "print('\\nTest set score:', rfc.score(X_bow_test, y_bow_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7418952618453866\n",
      "\n",
      "Test set score: 0.7581908831908832\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "tfidf_train = rfc.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_tfidf_train, y_tfidf_train))\n",
    "print('\\nTest set score:', rfc.score(X_tfidf_test, y_tfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9481481481481482\n",
      "\n",
      "Test set score: 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression() \n",
    "\n",
    "#bow\n",
    "bow_train = lr.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_bow_train, y_bow_train))\n",
    "print('\\nTest set score:', lr.score(X_bow_test, y_bow_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7410046312789454\n",
      "\n",
      "Test set score: 0.7549857549857549\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "tfidf_train = lr.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_tfidf_train, y_tfidf_train))\n",
    "print('\\nTest set score:', lr.score(X_tfidf_test, y_tfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9518518518518518\n",
      "\n",
      "Test set score: 0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "#bow\n",
    "bow_train = clf.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_bow_train, y_bow_train))\n",
    "print('\\nTest set score:', clf.score(X_bow_test, y_bow_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7353045956537229\n",
      "\n",
      "Test set score: 0.7535612535612536\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "tfidf_train = clf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_tfidf_train, y_tfidf_train))\n",
    "print('\\nTest set score:', clf.score(X_tfidf_test, y_tfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier - BOW\n",
      "Cross Validation Score: [0.77, 0.71, 0.88, 0.76, 0.79, 0.94, 0.74, 0.82, 0.76, 0.91]\n",
      "Average: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import statistics\n",
    "\n",
    "cvs_rfc_bow = cross_val_score(rfc, x_bow, y_bow, cv=10)\n",
    "cvs_rfc_bow = [\"%.2f\"%i for i in cvs_rfc_bow]\n",
    "cvs_rfc_bow = list(map(float, cvs_rfc_bow))\n",
    "\n",
    "print('Random Forest Classifier - BOW')\n",
    "print('Cross Validation Score: {}'.format(cvs_rfc_bow))\n",
    "print('Average: {0:.2f}'.format(statistics.mean(cvs_rfc_bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier - TFIDF\n",
      "Cross Validation Score: [0.74, 0.74, 0.75, 0.74, 0.73, 0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "Average: 0.74\n"
     ]
    }
   ],
   "source": [
    "cvs_rfc_tfidf = cross_val_score(rfc, x_tfidf, y_tfidf, cv=10)\n",
    "cvs_rfc_tfidf = [\"%.2f\"%i for i in cvs_rfc_tfidf]\n",
    "cvs_rfc_tfidf = list(map(float, cvs_rfc_tfidf))\n",
    "\n",
    "print('Random Forest Classifier - TFIDF')\n",
    "print('Cross Validation Score: {}'.format(cvs_rfc_tfidf))\n",
    "print('Average: {0:.2f}'.format(statistics.mean(cvs_rfc_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - BOW\n",
      "Cross Validation Score: [0.8, 0.76, 0.85, 0.76, 0.82, 0.94, 0.68, 0.85, 0.88, 0.94]\n",
      "Average: 0.83\n"
     ]
    }
   ],
   "source": [
    "cvs_lr_bow = cross_val_score(lr, x_bow, y_bow, cv=10)\n",
    "cvs_lr_bow = [\"%.2f\"%i for i in cvs_lr_bow]\n",
    "cvs_lr_bow = list(map(float, cvs_lr_bow))\n",
    "\n",
    "print('Logistic Regression - BOW')\n",
    "print('Cross Validation Score: {}'.format(cvs_lr_bow))\n",
    "print('Average: {0:.2f}'.format(statistics.mean(cvs_lr_bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - TFIDF\n",
      "Cross Validation Score: [0.74, 0.74, 0.75, 0.74, 0.73, 0.74, 0.74, 0.74, 0.73, 0.74]\n",
      "Average: 0.74\n"
     ]
    }
   ],
   "source": [
    "cvs_lr_tfidf = cross_val_score(lr, x_tfidf, y_tfidf, cv=10)\n",
    "cvs_lr_tfidf = [\"%.2f\"%i for i in cvs_lr_tfidf]\n",
    "cvs_lr_tfidf = list(map(float, cvs_lr_tfidf))\n",
    "\n",
    "print('Logistic Regression - TFIDF')\n",
    "print('Cross Validation Score: {}'.format(cvs_lr_tfidf))\n",
    "print('Average: {0:.2f}'.format(statistics.mean(cvs_lr_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - BOW\n",
      "Cross Validation Score: [0.71, 0.71, 0.82, 0.71, 0.82, 0.88, 0.68, 0.82, 0.91, 0.85]\n",
      "Average: 0.79\n"
     ]
    }
   ],
   "source": [
    "cvs_clf_bow = cross_val_score(clf, x_bow, y_bow, cv=10)\n",
    "cvs_clf_bow = [\"%.2f\"%i for i in cvs_clf_bow]\n",
    "cvs_clf_bow = list(map(float, cvs_clf_bow))\n",
    "\n",
    "print('Gradient Boosting - BOW')\n",
    "print('Cross Validation Score: {}'.format(cvs_clf_bow))\n",
    "print('Average: {0:.2f}'.format(statistics.mean(cvs_clf_bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - TFIDF\n",
      "Cross Validation Score: [0.74, 0.73, 0.74, 0.74, 0.73, 0.74, 0.74, 0.74, 0.73, 0.74]\n",
      "Average: 0.74\n"
     ]
    }
   ],
   "source": [
    "cvs_clf_tfidf = cross_val_score(clf, x_tfidf, y_tfidf, cv=10)\n",
    "cvs_clf_tfidf = [\"%.2f\"%i for i in cvs_clf_tfidf]\n",
    "cvs_clf_tfidf = list(map(float, cvs_clf_tfidf))\n",
    "\n",
    "print('Gradient Boosting - TFIDF')\n",
    "print('Cross Validation Score: {}'.format(cvs_clf_tfidf))\n",
    "print('Average: {0:.2f}'.format(statistics.mean(cvs_clf_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid-Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={\n",
    "    'n_estimators': (10, 50, 100, 1000),\n",
    "    'max_depth': range(2,7),\n",
    "    'min_samples_split': range(2,5)\n",
    "        }\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_grid = param_grid,\n",
    "    cv=10, \n",
    "    verbose=0, \n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "grid_result = gsc.fit(x_bow, y_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'min_samples_split': 2, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_result.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8333333333333334\n",
      "\n",
      "Test set score: 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "# Model using grid search best paramters\n",
    "rfc_bp = ensemble.RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params['min_samples_split']\n",
    "    ,random_state=1\n",
    ")\n",
    "\n",
    "#bow\n",
    "bow_train = rfc_bp.fit(X_bow_train, y_bow_train)\n",
    "\n",
    "print('Training set score:', rfc_bp.score(X_bow_train, y_bow_train))\n",
    "print('\\nTest set score:', rfc_bp.score(X_bow_test, y_bow_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
