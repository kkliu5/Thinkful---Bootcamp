{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# First Run (Yelp Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 432\n"
     ]
    }
   ],
   "source": [
    "# Import the raw data.\n",
    "df = pd.read_csv('/Users/Kevin/Files/Thinkful/Data Files/sentiment labelled sentences/yelp_labelled.txt', sep=\"\t\", header=None)\n",
    "df.columns = [\"review\", \"posneg\"]\n",
    "\n",
    "# creating pos and neg columns\n",
    "df['pos'] = (df['posneg'] == 1)\n",
    "df['neg'] = (df['posneg'] == 0)\n",
    "\n",
    "# making reviews lowercase to create keyword features columns\n",
    "df['review'].str.lower();\n",
    "\n",
    "#Adding keyword features\n",
    "# other keywords: ('wow', 'correct', 'love', 'best', 'great', 'recommend', 'excellent')\n",
    "pos_keywords = ['good', 'amazing']\n",
    "\n",
    "#creating each keyword feature\n",
    "for key in pos_keywords:\n",
    "    df[str(key)] = df.review.str.contains(str(key),case=False)\n",
    "    \n",
    "#Variables for model\n",
    "data = df[pos_keywords]\n",
    "target = df['pos']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores = [0.64 0.69 0.69 0.73 0.62 0.61 0.67 0.66 0.65 0.69]\n",
      "\n",
      "Confusion Matrix = \n",
      " [[458  42]\n",
      " [289 211]]\n",
      "\n",
      "Total = 1000\n",
      "Total Yes = 669\n",
      "Total No = 331\n",
      "\n",
      "Actual Yes = 500\n",
      "Actual No = 500\n",
      "Predicted Yes = 253\n",
      "Predicted No = 747\n",
      "\n",
      "TP = 211\n",
      "TN = 458\n",
      "FP = 42\n",
      "FN = 289\n",
      "\n",
      "Accuracy = 0.67\n",
      "Error Rate = 0.33\n",
      "Sensitivity = 0.42\n",
      "False Positive Rate = 0.084\n",
      "Specificity = 0.92\n",
      "Precision = 0.83\n",
      "Prevalance = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cvs = cross_val_score(bnb, data, target, cv=10)\n",
    "print('Cross Validation Scores = {}'.format(cvs))\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_mx = confusion_matrix(target, y_pred)\n",
    "\n",
    "print('Confusion Matrix = \\n {}'.format(cf_mx))\n",
    "\n",
    "print()\n",
    "\n",
    "#total\n",
    "total = data.shape[0]\n",
    "print('Total = {}'.format(total))\n",
    "\n",
    "#total yes \n",
    "total_yes = (target == y_pred).sum()\n",
    "print('Total Yes = {}'.format(total_yes))\n",
    "\n",
    "#total no\n",
    "total_no = (target != y_pred).sum()\n",
    "print('Total No = {}'.format(total_no))\n",
    "\n",
    "print()\n",
    "\n",
    "# Total Actual - Yes\n",
    "actual_yes = (target == True).sum()\n",
    "print('Actual Yes = {}'.format(actual_yes))\n",
    "\n",
    "# Total Actual - No\n",
    "actual_no = (target == False).sum()\n",
    "print('Actual No = {}'.format(actual_no))\n",
    "\n",
    "# Total Prediction - Yes\n",
    "predicted_yes = (y_pred == True).sum()\n",
    "print('Predicted Yes = {}'.format(predicted_yes))\n",
    "\n",
    "# Total Prediction - No\n",
    "predicted_no = (y_pred == False).sum()\n",
    "print('Predicted No = {}'.format(predicted_no))\n",
    "\n",
    "print()\n",
    "\n",
    "#true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "tp = np.logical_and(target == True, target == y_pred).sum()\n",
    "print('TP = {}'.format(tp))\n",
    "\n",
    "#true negatives (TN): We predicted no, and they don't have the disease.\n",
    "tn = np.logical_and(target == False, target == y_pred).sum()\n",
    "print('TN = {}'.format(tn))\n",
    "\n",
    "#false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "fp = np.logical_and(target == False, target != y_pred).sum()\n",
    "print('FP = {}'.format(fp))\n",
    "\n",
    "#false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "fn = np.logical_and(target == True, target != y_pred).sum()\n",
    "print('FN = {}'.format(fn))\n",
    "\n",
    "print()\n",
    "\n",
    "#Accuracy: Overall, how often is the classifier correct?\n",
    "#(TP+TN)/total \n",
    "accuracy = (tp + tn)/total\n",
    "print('Accuracy = {:.{prec}}'.format(accuracy, prec=2))\n",
    "\n",
    "#Misclassification Rate (\"Error Rate\"): Overall, how often is it wrong? (equivalent to 1 minus Accuracy) \n",
    "#(FP+FN)/total \n",
    "error_rate = (fp + fn)/ total\n",
    "print('Error Rate = {:.{prec}}'.format(error_rate, prec=2))\n",
    "\n",
    "#True Positive Rate (\"Sensitivity\"): When it's actually yes, how often does it predict yes? (also known as \"Recall\")\n",
    "#TP/actual yes \n",
    "sensitivity = tp / actual_yes\n",
    "print('Sensitivity = {:.{prec}}'.format(sensitivity, prec=2))\n",
    "\n",
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no \n",
    "false_positive = fp / actual_no\n",
    "print('False Positive Rate = {:.{prec}}'.format(false_positive, prec=2))\n",
    "\n",
    "#True Negative Rate(\"Specificity\"): When it's actually no, how often does it predict no? (equivalent to 1 minus False Positive Rate)\n",
    "#TN/actual no \n",
    "specificity = tn / actual_no\n",
    "print('Specificity = {:.{prec}}'.format(specificity, prec=2))\n",
    "\n",
    "#Precision: When it predicts yes, how often is it correct?\n",
    "#TP/predicted yes \n",
    "precision = tp / predicted_yes\n",
    "print('Precision = {:.{prec}}'.format(precision, prec=2))\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total \n",
    "prevalence = actual_yes / total\n",
    "print('Prevalance = {:.{prec}}'.format(prevalence, prec=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Second Run (Yelp Reviews)\n",
    "\n",
    "Adding 'great' and 'recommend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 370\n"
     ]
    }
   ],
   "source": [
    "# Import the raw data.\n",
    "df = pd.read_csv('/Users/Kevin/Files/Thinkful/Data Files/sentiment labelled sentences/yelp_labelled.txt', sep=\"\t\", header=None)\n",
    "df.columns = [\"review\", \"posneg\"]\n",
    "\n",
    "# creating pos and neg columns\n",
    "df['pos'] = (df['posneg'] == 1)\n",
    "df['neg'] = (df['posneg'] == 0)\n",
    "\n",
    "# making reviews lowercase to create keyword features columns\n",
    "df['review'].str.lower();\n",
    "\n",
    "#Adding keyword features\n",
    "# other keywords: ('wow', 'correct', 'love', 'best', 'excellent')\n",
    "pos_keywords = ['good', 'amazing', 'great', 'recommend']\n",
    "\n",
    "#creating each keyword feature\n",
    "for key in pos_keywords:\n",
    "    df[str(key)] = df.review.str.contains(str(key),case=False)\n",
    "    \n",
    "#Variables for model\n",
    "data = df[pos_keywords]\n",
    "target = df['pos']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores = [0.64 0.69 0.69 0.73 0.62 0.61 0.67 0.66 0.65 0.69]\n",
      "\n",
      "Confusion Matrix = \n",
      " [[458  42]\n",
      " [289 211]]\n",
      "\n",
      "Total = 1000\n",
      "Total Yes = 669\n",
      "Total No = 331\n",
      "\n",
      "Actual Yes = 500\n",
      "Actual No = 500\n",
      "Predicted Yes = 253\n",
      "Predicted No = 747\n",
      "\n",
      "TP = 211\n",
      "TN = 458\n",
      "FP = 42\n",
      "FN = 289\n",
      "\n",
      "Accuracy = 0.67\n",
      "Error Rate = 0.33\n",
      "Sensitivity = 0.42\n",
      "False Positive Rate = 0.084\n",
      "Specificity = 0.92\n",
      "Precision = 0.83\n",
      "Prevalance = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cvs = cross_val_score(bnb, data, target, cv=10)\n",
    "print('Cross Validation Scores = {}'.format(cvs))\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_mx = confusion_matrix(target, y_pred)\n",
    "\n",
    "print('Confusion Matrix = \\n {}'.format(cf_mx))\n",
    "\n",
    "print()\n",
    "\n",
    "#total\n",
    "total = data.shape[0]\n",
    "print('Total = {}'.format(total))\n",
    "\n",
    "#total yes \n",
    "total_yes = (target == y_pred).sum()\n",
    "print('Total Yes = {}'.format(total_yes))\n",
    "\n",
    "#total no\n",
    "total_no = (target != y_pred).sum()\n",
    "print('Total No = {}'.format(total_no))\n",
    "\n",
    "print()\n",
    "\n",
    "# Total Actual - Yes\n",
    "actual_yes = (target == True).sum()\n",
    "print('Actual Yes = {}'.format(actual_yes))\n",
    "\n",
    "# Total Actual - No\n",
    "actual_no = (target == False).sum()\n",
    "print('Actual No = {}'.format(actual_no))\n",
    "\n",
    "# Total Prediction - Yes\n",
    "predicted_yes = (y_pred == True).sum()\n",
    "print('Predicted Yes = {}'.format(predicted_yes))\n",
    "\n",
    "# Total Prediction - No\n",
    "predicted_no = (y_pred == False).sum()\n",
    "print('Predicted No = {}'.format(predicted_no))\n",
    "\n",
    "print()\n",
    "\n",
    "#true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "tp = np.logical_and(target == True, target == y_pred).sum()\n",
    "print('TP = {}'.format(tp))\n",
    "\n",
    "#true negatives (TN): We predicted no, and they don't have the disease.\n",
    "tn = np.logical_and(target == False, target == y_pred).sum()\n",
    "print('TN = {}'.format(tn))\n",
    "\n",
    "#false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "fp = np.logical_and(target == False, target != y_pred).sum()\n",
    "print('FP = {}'.format(fp))\n",
    "\n",
    "#false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "fn = np.logical_and(target == True, target != y_pred).sum()\n",
    "print('FN = {}'.format(fn))\n",
    "\n",
    "print()\n",
    "\n",
    "#Accuracy: Overall, how often is the classifier correct?\n",
    "#(TP+TN)/total \n",
    "accuracy = (tp + tn)/total\n",
    "print('Accuracy = {:.{prec}}'.format(accuracy, prec=2))\n",
    "\n",
    "#Misclassification Rate (\"Error Rate\"): Overall, how often is it wrong? (equivalent to 1 minus Accuracy) \n",
    "#(FP+FN)/total \n",
    "error_rate = (fp + fn)/ total\n",
    "print('Error Rate = {:.{prec}}'.format(error_rate, prec=2))\n",
    "\n",
    "#True Positive Rate (\"Sensitivity\"): When it's actually yes, how often does it predict yes? (also known as \"Recall\")\n",
    "#TP/actual yes \n",
    "sensitivity = tp / actual_yes\n",
    "print('Sensitivity = {:.{prec}}'.format(sensitivity, prec=2))\n",
    "\n",
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no \n",
    "false_positive = fp / actual_no\n",
    "print('False Positive Rate = {:.{prec}}'.format(false_positive, prec=2))\n",
    "\n",
    "#True Negative Rate(\"Specificity\"): When it's actually no, how often does it predict no? (equivalent to 1 minus False Positive Rate)\n",
    "#TN/actual no \n",
    "specificity = tn / actual_no\n",
    "print('Specificity = {:.{prec}}'.format(specificity, prec=2))\n",
    "\n",
    "#Precision: When it predicts yes, how often is it correct?\n",
    "#TP/predicted yes \n",
    "precision = tp / predicted_yes\n",
    "print('Precision = {:.{prec}}'.format(precision, prec=2))\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total \n",
    "prevalence = actual_yes / total\n",
    "print('Prevalance = {:.{prec}}'.format(prevalence, prec=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Third Run (Yelp Reviews)\n",
    "\n",
    "Adding 'best' and 'excellent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 353\n"
     ]
    }
   ],
   "source": [
    "# Import the raw data.\n",
    "df = pd.read_csv('/Users/Kevin/Files/Thinkful/Data Files/sentiment labelled sentences/yelp_labelled.txt', sep=\"\t\", header=None)\n",
    "df.columns = [\"review\", \"posneg\"]\n",
    "\n",
    "# creating pos and neg columns\n",
    "df['pos'] = (df['posneg'] == 1)\n",
    "df['neg'] = (df['posneg'] == 0)\n",
    "\n",
    "# making reviews lowercase to create keyword features columns\n",
    "df['review'].str.lower();\n",
    "\n",
    "#Adding keyword features\n",
    "# other keywords: ('wow', 'correct', 'love')\n",
    "pos_keywords = ['good', 'amazing', 'great', 'recommend', 'best', 'excellent']\n",
    "\n",
    "#creating each keyword feature\n",
    "for key in pos_keywords:\n",
    "    df[str(key)] = df.review.str.contains(str(key),case=False)\n",
    "    \n",
    "#Variables for model\n",
    "data = df[pos_keywords]\n",
    "target = df['pos']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores = [0.64 0.69 0.69 0.73 0.62 0.61 0.67 0.66 0.65 0.69]\n",
      "\n",
      "Confusion Matrix = \n",
      " [[458  42]\n",
      " [289 211]]\n",
      "\n",
      "Total = 1000\n",
      "Total Yes = 669\n",
      "Total No = 331\n",
      "\n",
      "Actual Yes = 500\n",
      "Actual No = 500\n",
      "Predicted Yes = 253\n",
      "Predicted No = 747\n",
      "\n",
      "TP = 211\n",
      "TN = 458\n",
      "FP = 42\n",
      "FN = 289\n",
      "\n",
      "Accuracy = 0.67\n",
      "Error Rate = 0.33\n",
      "Sensitivity = 0.42\n",
      "False Positive Rate = 0.084\n",
      "Specificity = 0.92\n",
      "Precision = 0.83\n",
      "Prevalance = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cvs = cross_val_score(bnb, data, target, cv=10)\n",
    "print('Cross Validation Scores = {}'.format(cvs))\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_mx = confusion_matrix(target, y_pred)\n",
    "\n",
    "print('Confusion Matrix = \\n {}'.format(cf_mx))\n",
    "\n",
    "print()\n",
    "\n",
    "#total\n",
    "total = data.shape[0]\n",
    "print('Total = {}'.format(total))\n",
    "\n",
    "#total yes \n",
    "total_yes = (target == y_pred).sum()\n",
    "print('Total Yes = {}'.format(total_yes))\n",
    "\n",
    "#total no\n",
    "total_no = (target != y_pred).sum()\n",
    "print('Total No = {}'.format(total_no))\n",
    "\n",
    "print()\n",
    "\n",
    "# Total Actual - Yes\n",
    "actual_yes = (target == True).sum()\n",
    "print('Actual Yes = {}'.format(actual_yes))\n",
    "\n",
    "# Total Actual - No\n",
    "actual_no = (target == False).sum()\n",
    "print('Actual No = {}'.format(actual_no))\n",
    "\n",
    "# Total Prediction - Yes\n",
    "predicted_yes = (y_pred == True).sum()\n",
    "print('Predicted Yes = {}'.format(predicted_yes))\n",
    "\n",
    "# Total Prediction - No\n",
    "predicted_no = (y_pred == False).sum()\n",
    "print('Predicted No = {}'.format(predicted_no))\n",
    "\n",
    "print()\n",
    "\n",
    "#true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "tp = np.logical_and(target == True, target == y_pred).sum()\n",
    "print('TP = {}'.format(tp))\n",
    "\n",
    "#true negatives (TN): We predicted no, and they don't have the disease.\n",
    "tn = np.logical_and(target == False, target == y_pred).sum()\n",
    "print('TN = {}'.format(tn))\n",
    "\n",
    "#false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "fp = np.logical_and(target == False, target != y_pred).sum()\n",
    "print('FP = {}'.format(fp))\n",
    "\n",
    "#false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "fn = np.logical_and(target == True, target != y_pred).sum()\n",
    "print('FN = {}'.format(fn))\n",
    "\n",
    "print()\n",
    "\n",
    "#Accuracy: Overall, how often is the classifier correct?\n",
    "#(TP+TN)/total \n",
    "accuracy = (tp + tn)/total\n",
    "print('Accuracy = {:.{prec}}'.format(accuracy, prec=2))\n",
    "\n",
    "#Misclassification Rate (\"Error Rate\"): Overall, how often is it wrong? (equivalent to 1 minus Accuracy) \n",
    "#(FP+FN)/total \n",
    "error_rate = (fp + fn)/ total\n",
    "print('Error Rate = {:.{prec}}'.format(error_rate, prec=2))\n",
    "\n",
    "#True Positive Rate (\"Sensitivity\"): When it's actually yes, how often does it predict yes? (also known as \"Recall\")\n",
    "#TP/actual yes \n",
    "sensitivity = tp / actual_yes\n",
    "print('Sensitivity = {:.{prec}}'.format(sensitivity, prec=2))\n",
    "\n",
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no \n",
    "false_positive = fp / actual_no\n",
    "print('False Positive Rate = {:.{prec}}'.format(false_positive, prec=2))\n",
    "\n",
    "#True Negative Rate(\"Specificity\"): When it's actually no, how often does it predict no? (equivalent to 1 minus False Positive Rate)\n",
    "#TN/actual no \n",
    "specificity = tn / actual_no\n",
    "print('Specificity = {:.{prec}}'.format(specificity, prec=2))\n",
    "\n",
    "#Precision: When it predicts yes, how often is it correct?\n",
    "#TP/predicted yes \n",
    "precision = tp / predicted_yes\n",
    "print('Precision = {:.{prec}}'.format(precision, prec=2))\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total \n",
    "prevalence = actual_yes / total\n",
    "print('Prevalance = {:.{prec}}'.format(prevalence, prec=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Fourth Run (Yelp Reviews)\n",
    "\n",
    "Adding 'correct' and 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 353\n"
     ]
    }
   ],
   "source": [
    "# Import the raw data.\n",
    "df = pd.read_csv('/Users/Kevin/Files/Thinkful/Data Files/sentiment labelled sentences/yelp_labelled.txt', sep=\"\t\", header=None)\n",
    "df.columns = [\"review\", \"posneg\"]\n",
    "\n",
    "# creating pos and neg columns\n",
    "df['pos'] = (df['posneg'] == 1)\n",
    "df['neg'] = (df['posneg'] == 0)\n",
    "\n",
    "# making reviews lowercase to create keyword features columns\n",
    "df['review'].str.lower();\n",
    "\n",
    "#Adding keyword features\n",
    "# other keywords: ('wow', 'love')\n",
    "pos_keywords = ['good', 'amazing', 'great', 'recommend', 'best', 'excellent', 'correct', 'yes']\n",
    "\n",
    "#creating each keyword feature\n",
    "for key in pos_keywords:\n",
    "    df[str(key)] = df.review.str.contains(str(key),case=False)\n",
    "    \n",
    "#Variables for model\n",
    "data = df[pos_keywords]\n",
    "target = df['pos']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores = [0.64 0.69 0.69 0.73 0.62 0.61 0.67 0.66 0.65 0.69]\n",
      "\n",
      "Confusion Matrix = \n",
      " [[458  42]\n",
      " [289 211]]\n",
      "\n",
      "Total = 1000\n",
      "Total Yes = 669\n",
      "Total No = 331\n",
      "\n",
      "Actual Yes = 500\n",
      "Actual No = 500\n",
      "Predicted Yes = 253\n",
      "Predicted No = 747\n",
      "\n",
      "TP = 211\n",
      "TN = 458\n",
      "FP = 42\n",
      "FN = 289\n",
      "\n",
      "Accuracy = 0.67\n",
      "Error Rate = 0.33\n",
      "Sensitivity = 0.42\n",
      "False Positive Rate = 0.084\n",
      "Specificity = 0.92\n",
      "Precision = 0.83\n",
      "Prevalance = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cvs = cross_val_score(bnb, data, target, cv=10)\n",
    "print('Cross Validation Scores = {}'.format(cvs))\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_mx = confusion_matrix(target, y_pred)\n",
    "\n",
    "print('Confusion Matrix = \\n {}'.format(cf_mx))\n",
    "\n",
    "print()\n",
    "\n",
    "#total\n",
    "total = data.shape[0]\n",
    "print('Total = {}'.format(total))\n",
    "\n",
    "#total yes \n",
    "total_yes = (target == y_pred).sum()\n",
    "print('Total Yes = {}'.format(total_yes))\n",
    "\n",
    "#total no\n",
    "total_no = (target != y_pred).sum()\n",
    "print('Total No = {}'.format(total_no))\n",
    "\n",
    "print()\n",
    "\n",
    "# Total Actual - Yes\n",
    "actual_yes = (target == True).sum()\n",
    "print('Actual Yes = {}'.format(actual_yes))\n",
    "\n",
    "# Total Actual - No\n",
    "actual_no = (target == False).sum()\n",
    "print('Actual No = {}'.format(actual_no))\n",
    "\n",
    "# Total Prediction - Yes\n",
    "predicted_yes = (y_pred == True).sum()\n",
    "print('Predicted Yes = {}'.format(predicted_yes))\n",
    "\n",
    "# Total Prediction - No\n",
    "predicted_no = (y_pred == False).sum()\n",
    "print('Predicted No = {}'.format(predicted_no))\n",
    "\n",
    "print()\n",
    "\n",
    "#true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "tp = np.logical_and(target == True, target == y_pred).sum()\n",
    "print('TP = {}'.format(tp))\n",
    "\n",
    "#true negatives (TN): We predicted no, and they don't have the disease.\n",
    "tn = np.logical_and(target == False, target == y_pred).sum()\n",
    "print('TN = {}'.format(tn))\n",
    "\n",
    "#false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "fp = np.logical_and(target == False, target != y_pred).sum()\n",
    "print('FP = {}'.format(fp))\n",
    "\n",
    "#false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "fn = np.logical_and(target == True, target != y_pred).sum()\n",
    "print('FN = {}'.format(fn))\n",
    "\n",
    "print()\n",
    "\n",
    "#Accuracy: Overall, how often is the classifier correct?\n",
    "#(TP+TN)/total \n",
    "accuracy = (tp + tn)/total\n",
    "print('Accuracy = {:.{prec}}'.format(accuracy, prec=2))\n",
    "\n",
    "#Misclassification Rate (\"Error Rate\"): Overall, how often is it wrong? (equivalent to 1 minus Accuracy) \n",
    "#(FP+FN)/total \n",
    "error_rate = (fp + fn)/ total\n",
    "print('Error Rate = {:.{prec}}'.format(error_rate, prec=2))\n",
    "\n",
    "#True Positive Rate (\"Sensitivity\"): When it's actually yes, how often does it predict yes? (also known as \"Recall\")\n",
    "#TP/actual yes \n",
    "sensitivity = tp / actual_yes\n",
    "print('Sensitivity = {:.{prec}}'.format(sensitivity, prec=2))\n",
    "\n",
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no \n",
    "false_positive = fp / actual_no\n",
    "print('False Positive Rate = {:.{prec}}'.format(false_positive, prec=2))\n",
    "\n",
    "#True Negative Rate(\"Specificity\"): When it's actually no, how often does it predict no? (equivalent to 1 minus False Positive Rate)\n",
    "#TN/actual no \n",
    "specificity = tn / actual_no\n",
    "print('Specificity = {:.{prec}}'.format(specificity, prec=2))\n",
    "\n",
    "#Precision: When it predicts yes, how often is it correct?\n",
    "#TP/predicted yes \n",
    "precision = tp / predicted_yes\n",
    "print('Precision = {:.{prec}}'.format(precision, prec=2))\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total \n",
    "prevalence = actual_yes / total\n",
    "print('Prevalance = {:.{prec}}'.format(prevalence, prec=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Fifth Run (Yelp Reviews)\n",
    "\n",
    "Adding 'delicious' and 'tasty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 331\n"
     ]
    }
   ],
   "source": [
    "# Import the raw data.\n",
    "df = pd.read_csv('/Users/Kevin/Files/Thinkful/Data Files/sentiment labelled sentences/yelp_labelled.txt', sep=\"\t\", header=None)\n",
    "df.columns = [\"review\", \"posneg\"]\n",
    "\n",
    "# creating pos and neg columns\n",
    "df['pos'] = (df['posneg'] == 1)\n",
    "df['neg'] = (df['posneg'] == 0)\n",
    "\n",
    "# making reviews lowercase to create keyword features columns\n",
    "df['review'].str.lower();\n",
    "\n",
    "#Adding keyword features\n",
    "# other keywords: ('wow', 'love')\n",
    "pos_keywords = ['good', 'amazing', 'great', 'recommend', 'best', 'excellent', 'correct', 'yes', 'delicious', 'tasty']\n",
    "\n",
    "#creating each keyword feature\n",
    "for key in pos_keywords:\n",
    "    df[str(key)] = df.review.str.contains(str(key),case=False)\n",
    "    \n",
    "#Variables for model\n",
    "data = df[pos_keywords]\n",
    "target = df['pos']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores = [0.64 0.69 0.69 0.73 0.62 0.61 0.67 0.66 0.65 0.69]\n",
      "\n",
      "Confusion Matrix = \n",
      " [[458  42]\n",
      " [289 211]]\n",
      "\n",
      "Total = 1000\n",
      "Total Yes = 669\n",
      "Total No = 331\n",
      "\n",
      "Actual Yes = 500\n",
      "Actual No = 500\n",
      "Predicted Yes = 253\n",
      "Predicted No = 747\n",
      "\n",
      "TP = 211\n",
      "TN = 458\n",
      "FP = 42\n",
      "FN = 289\n",
      "\n",
      "Accuracy = 0.67\n",
      "Error Rate = 0.33\n",
      "Sensitivity = 0.42\n",
      "False Positive Rate = 0.084\n",
      "Specificity = 0.92\n",
      "Precision = 0.83\n",
      "Prevalance = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cvs = cross_val_score(bnb, data, target, cv=10)\n",
    "print('Cross Validation Scores = {}'.format(cvs))\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_mx = confusion_matrix(target, y_pred)\n",
    "\n",
    "print('Confusion Matrix = \\n {}'.format(cf_mx))\n",
    "\n",
    "print()\n",
    "\n",
    "#total\n",
    "total = data.shape[0]\n",
    "print('Total = {}'.format(total))\n",
    "\n",
    "#total yes \n",
    "total_yes = (target == y_pred).sum()\n",
    "print('Total Yes = {}'.format(total_yes))\n",
    "\n",
    "#total no\n",
    "total_no = (target != y_pred).sum()\n",
    "print('Total No = {}'.format(total_no))\n",
    "\n",
    "print()\n",
    "\n",
    "# Total Actual - Yes\n",
    "actual_yes = (target == True).sum()\n",
    "print('Actual Yes = {}'.format(actual_yes))\n",
    "\n",
    "# Total Actual - No\n",
    "actual_no = (target == False).sum()\n",
    "print('Actual No = {}'.format(actual_no))\n",
    "\n",
    "# Total Prediction - Yes\n",
    "predicted_yes = (y_pred == True).sum()\n",
    "print('Predicted Yes = {}'.format(predicted_yes))\n",
    "\n",
    "# Total Prediction - No\n",
    "predicted_no = (y_pred == False).sum()\n",
    "print('Predicted No = {}'.format(predicted_no))\n",
    "\n",
    "print()\n",
    "\n",
    "#true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "tp = np.logical_and(target == True, target == y_pred).sum()\n",
    "print('TP = {}'.format(tp))\n",
    "\n",
    "#true negatives (TN): We predicted no, and they don't have the disease.\n",
    "tn = np.logical_and(target == False, target == y_pred).sum()\n",
    "print('TN = {}'.format(tn))\n",
    "\n",
    "#false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "fp = np.logical_and(target == False, target != y_pred).sum()\n",
    "print('FP = {}'.format(fp))\n",
    "\n",
    "#false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "fn = np.logical_and(target == True, target != y_pred).sum()\n",
    "print('FN = {}'.format(fn))\n",
    "\n",
    "print()\n",
    "\n",
    "#Accuracy: Overall, how often is the classifier correct?\n",
    "#(TP+TN)/total \n",
    "accuracy = (tp + tn)/total\n",
    "print('Accuracy = {:.{prec}}'.format(accuracy, prec=2))\n",
    "\n",
    "#Misclassification Rate (\"Error Rate\"): Overall, how often is it wrong? (equivalent to 1 minus Accuracy) \n",
    "#(FP+FN)/total \n",
    "error_rate = (fp + fn)/ total\n",
    "print('Error Rate = {:.{prec}}'.format(error_rate, prec=2))\n",
    "\n",
    "#True Positive Rate (\"Sensitivity\"): When it's actually yes, how often does it predict yes? (also known as \"Recall\")\n",
    "#TP/actual yes \n",
    "sensitivity = tp / actual_yes\n",
    "print('Sensitivity = {:.{prec}}'.format(sensitivity, prec=2))\n",
    "\n",
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no \n",
    "false_positive = fp / actual_no\n",
    "print('False Positive Rate = {:.{prec}}'.format(false_positive, prec=2))\n",
    "\n",
    "#True Negative Rate(\"Specificity\"): When it's actually no, how often does it predict no? (equivalent to 1 minus False Positive Rate)\n",
    "#TN/actual no \n",
    "specificity = tn / actual_no\n",
    "print('Specificity = {:.{prec}}'.format(specificity, prec=2))\n",
    "\n",
    "#Precision: When it predicts yes, how often is it correct?\n",
    "#TP/predicted yes \n",
    "precision = tp / predicted_yes\n",
    "print('Precision = {:.{prec}}'.format(precision, prec=2))\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total \n",
    "prevalence = actual_yes / total\n",
    "print('Prevalance = {:.{prec}}'.format(prevalence, prec=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Overview\n",
    "\n",
    "Through each iteration except for the fourth, the accuracy and the sensitivity of the model improved. However the specificity of the model decreased slightly over each iteration. \n",
    "\n",
    "Accuracy_1 = .57 <br>\n",
    "Accuracy_2 = .63 <br>\n",
    "Accuracy_3 = .65 <br>\n",
    "Accuracy_4 = .65 <br>\n",
    "Accuracy_5 = .67 <br>\n",
    "\n",
    "Sensitivity_1 = .18 <br>\n",
    "Sensitivity_2 = .32 <br>\n",
    "Sensitivity_3 = .37 <br>\n",
    "Sensitivity_4 = .37 <br>\n",
    "Sensitivity_5 = .42 <br>\n",
    "\n",
    "Specificity_1 = .95 <br>\n",
    "Specificity_1 = .94 <br>\n",
    "Specificity_1 = .92 <br>\n",
    "Specificity_1 = .92 <br>\n",
    "Specificity_1 = .92 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do any of your classifiers seem to overfit?\n",
    "- I do not believe any of the classifiers overfit. After looking at each cross validation they are all generally in-line.\n",
    "\n",
    "Which seem to perform the best? Why?\n",
    "- The second run performed the best with the accuracy increasing by 6% and sensitivity increasing by 14%. Adding \"great\" and \"recommend\" improved the scores.\n",
    "\n",
    "Which features seemed to be most impactful to performance?\n",
    "- Adding \"great\" and \"recommend\" improved the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
