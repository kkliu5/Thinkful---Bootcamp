{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill: tf-idf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sentences:\n",
    "\n",
    "1. \"The best Monty Python sketch is the one about the dead parrot, I laughed so hard.\"\n",
    "2. \"I laugh when I think about Python's Ministry of Silly Walks sketch, it is funny, funny, funny, the best!\"\n",
    "3. \"Chocolate is the best ice cream dessert topping, with a great taste.\"\n",
    "4. \"The Lumberjack Song is the funniest Monty Python bit: I can't think of it without laughing.\"\n",
    "5. \"I would rather put strawberries on my ice cream for dessert, they have the best taste.\"\n",
    "6. \"The taste of caramel is a fantastic accompaniment to tasty mint ice cream.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | 1 | 2 | 3 | 4 | 5 | 6 | \n",
    "|-----------|---|---|---|---|---|---|\n",
    "| Monty | 1 | 0 | 0 | 1 | 0 | 0 |\n",
    "| Python | 1 | 1 | 0 | 1 | 0 | 0 |\n",
    "| sketch | 1 | 1 | 0 | 0 | 0 | 0 | \n",
    "| laugh | 1 | 1 | 0 | 1 | 0 | 0 | \n",
    "| funny | 0 | 3 | 0 | 1 | 0 | 0 | \n",
    "| best | 1 | 1 | 1 | 0 | 1 | 0 |\n",
    "| ice cream | 0 | 0 | 1 | 0 | 1 | 1 | \n",
    "| dessert | 0 | 0 | 1 | 0 | 1 | 0 |\n",
    "| taste | 0 | 0 | 1 | 0 | 1 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Document frequency' counts how many sentences a word appears in. \n",
    "\n",
    "'Collection frequency' counts how often a word appears, total, over all sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |df |cf|\n",
    "|-----------|---|---| \n",
    "| Monty | 2 | 2 | \n",
    "| Python | 3 | 3 |\n",
    "| sketch | 2 | 2 |\n",
    "| laugh | 3 | 3 | \n",
    "| funny | 2 | 4 | \n",
    "| best | 4 | 4 | \n",
    "| ice cream | 3 | 3 | \n",
    "| dessert | 2 | 2 |\n",
    "| taste | 3 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = total documents\n",
    "\n",
    "df = document frequency \n",
    "\n",
    "cf = collection frequency\n",
    "\n",
    "idf = inverse document frequency number\n",
    "\n",
    "t = term\n",
    "\n",
    "$$idf_t=log \\dfrac N{df_t}$$\n",
    "\n",
    "$$tf-idf_{t,d}=(tf_{t,d})(idf_t)$$\n",
    "\n",
    "Now we can represent each sentence as a vector made up of the tf-idf scores for each word:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | 1 | 2 | 3 | \n",
    "|-----------|---|---|---|\n",
    "| Monty | 1.585 | 0 | 0 |\n",
    "| Python | 1 | 1 | 0 |\n",
    "| sketch | 1.585| 1.585 | 0 | \n",
    "| laugh | 1 | 1 | 0 | \n",
    "| funny | 0 | 4.755 | 0 | \n",
    "| best | .585 | .585 | .585 | \n",
    "| ice cream | 0 | 0 | 1 | \n",
    "| dessert | 0 | 0 | 1.585 | \n",
    "| taste | 0 | 0 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = [1,1,0,1,1,0,0,0,0]\n",
    "l5 = [0,0,0,0,0,1,1,1,1]\n",
    "l6 = [0,0,0,0,0,0,1,0,2]\n",
    "\n",
    "df = [2,3,2,3,2,4,3,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf = []\n",
    "\n",
    "for i in df:\n",
    "    idf.append(math.log((6/i),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5849625007211563,\n",
       " 1.0,\n",
       " 1.5849625007211563,\n",
       " 1.0,\n",
       " 1.5849625007211563,\n",
       " 0.5849625007211562,\n",
       " 1.0,\n",
       " 1.5849625007211563,\n",
       " 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_4 = [a*b for a,b in zip(l4,idf)]\n",
    "tf_idf_5 = [a*b for a,b in zip(l5,idf)]\n",
    "tf_idf_6 = [a*b for a,b in zip(l6,idf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5849625007211563, 1.0, 0.0, 1.0, 1.5849625007211563, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.5849625007211562, 1.0, 1.5849625007211563, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_4)\n",
    "print(tf_idf_5)\n",
    "print(tf_idf_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same answers as answer key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill 0: Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Kevin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package gutenberg to /Users/Kevin/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I', 'Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1948\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "Tf_idf vector: {'minutes': array(0.71274503), 'present': array(0.70142321)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 49.392390804241394\n",
      "\n",
      "Component 0:\n",
      "\" Oh !       0.99992\n",
      "\" Oh no !    0.99992\n",
      "\" Oh no !    0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "\" Oh !       0.99992\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "\" Well , Mrs . Weston ,\" said Emma triumphantly when he left them , \" what do you say now to Mr . Knightley ' s marrying Jane Fairfax ?\"                                                                                                                                                                                                                                                                                                             0.612139\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                                                                                                                                                                                                                                                                                                                                                           0.600520\n",
      "Frank turned instantly to Emma , to claim her former promise ; and boasted himself an engaged man , which his father looked his most perfect approbation of  and it then appeared that Mrs . Weston was wanting _him_ to dance with Mrs . Elton himself , and that their business was to help to persuade him into it , which was done pretty soon . Mr . Weston and Mrs . Elton led the way , Mr . Frank Churchill and Miss Woodhouse followed .    0.570908\n",
      "\" Mr .                                                                                                                                                                                                                                                                                                                                                                                                                                               0.519101\n",
      "While she was gone , Mr . Knightley called , and sat some time with Mr . Woodhouse and Emma , till Mr . Woodhouse , who had previously made up his mind to walk out , was persuaded by his daughter not to defer it , and was induced by the entreaties of both , though against the scruples of his own civility , to leave Mr . Knightley for that purpose .                                                                                       0.511469\n",
      "Mr . Weston was musing .                                                                                                                                                                                                                                                                                                                                                                                                                             0.505236\n",
      "\" I think , indeed ,\" said John Knightley pleasantly , \" that Mr . Weston has some little claim .                                                                                                                                                                                                                                                                                                                                                    0.490660\n",
      "\" Mrs . Weston ' s manners ,\" said Emma , \" were always particularly good .                                                                                                                                                                                                                                                                                                                                                                          0.486134\n",
      "\" Why , to be sure ,\" said Mr . Woodhouse \" yes , certainly  I cannot deny that Mrs . Weston , poor Mrs . Weston , does come and see us pretty often  but then  she is always obliged to go away again .\"                                                                                                                                                                                                                                            0.486009\n",
      "\" It is Frank and Miss Fairfax ,\" said Mrs . Weston .                                                                                                                                                                                                                                                                                                                                                                                                0.485600\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "\" Ah !     0.996961\n",
      "\" Ah !     0.996961\n",
      "\" Ah !     0.996961\n",
      "\" Ah !\"    0.996961\n",
      "\" Ah !\"    0.996961\n",
      "\" Ah !     0.996961\n",
      "\" Ah !     0.996961\n",
      "\" Ah !     0.996961\n",
      "\" Ah !     0.996961\n",
      "\" Ah !     0.996961\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "\" Mr .                                                                                                                                                                                                                                                                 0.633419\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                                                                                                                                                                             0.564647\n",
      "Mr . Knightley was thoughtful again .                                                                                                                                                                                                                                  0.523580\n",
      "\" You are not vain , Mr . Knightley .                                                                                                                                                                                                                                  0.507846\n",
      "Mr . Weston was musing .                                                                                                                                                                                                                                               0.499847\n",
      "Mr . Weston ' s own happiness was indisputable .                                                                                                                                                                                                                       0.456819\n",
      "She meant to be very happy , in spite of the scene being laid at Mr . Cole ' s ; and without being able to forget that among the failings of Mr . Elton , even in the days of his favour , none had disturbed her more than his propensity to dine with Mr . Cole .    0.426147\n",
      "Harriet , Mr . Elton , and Mr . Knightley , their own especial set , were the only persons invited to meet them ; the hours were to be early , as well as the numbers few ; Mr . Woodhouse ' s habits and inclination being consulted in every thing .                 0.398089\n",
      "\" Christmas weather ,\" observed Mr . Elton .                                                                                                                                                                                                                           0.397338\n",
      "\" And I , Mr . Knightley , am equally stout in my confidence of its not doing them any harm .                                                                                                                                                                          0.391390\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "\" Yes .                                                                         0.652900\n",
      "\" Yes , do .\"                                                                   0.652900\n",
      "\" Yes , sometimes he can .\"                                                     0.652900\n",
      "\" Yes I should , I am sure I should .                                           0.638978\n",
      "\" Yes , so I imagined .                                                         0.611835\n",
      "\" Yes  a good deal _nearer_ .\"                                                  0.562223\n",
      "\" Yes , our good Mrs . Elton .                                                  0.553057\n",
      "\" Yes ; but we must not rest our claims on that distinction .                   0.550938\n",
      "\" Yes , very soon .                                                             0.534187\n",
      "\" Yes , she would be , but that she thinks there will be another put - off .    0.533841\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print(\"\")\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set picks up \"ah!\" and \"oh!\" like the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill 1: Tweaking tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1358\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "Tf_idf vector: {'minutes': array(0.71274503), 'present': array(0.70142321)}\n",
      "Percent variance captured by all components: 48.45414951322034\n",
      "Component 0:\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !\"    0.999239\n",
      "\" Oh !     0.999239\n",
      "\" Oh !     0.999239\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .                                                                                                                0.676580\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                 0.595194\n",
      "\" You get upon delicate subjects , Emma ,\" said Mrs . Weston smiling ; \" remember that I am here . Mr .                                                                     0.585153\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"    0.568883\n",
      "\" Emma ,\" said Mr . Knightley presently , \" I have a piece of news for you .                                                                                                0.545980\n",
      "Mr . Knightley might quarrel with her , but Emma could not quarrel with herself .                                                                                           0.544947\n",
      "\" I do not admire it ,\" said Mr . Knightley .                                                                                                                               0.543580\n",
      "\" Mr . Weston will be almost as much relieved as myself ,\" said she .                                                                                                       0.540942\n",
      "\" It is not now worth a regret ,\" said Emma .                                                                                                                               0.536581\n",
      "\" You are right , Mrs . Weston ,\" said Mr . Knightley warmly , \" Miss Fairfax is as capable as any of us of forming a just opinion of Mrs . Elton .                         0.531342\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "CHAPTER V      0.998958\n",
      "CHAPTER I      0.998958\n",
      "CHAPTER V      0.998958\n",
      "CHAPTER I      0.998958\n",
      "CHAPTER I      0.998958\n",
      "CHAPTER X      0.998958\n",
      "CHAPTER X      0.998958\n",
      "CHAPTER X      0.998958\n",
      "CHAPTER V      0.998958\n",
      "CHAPTER XII    0.998160\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .    0.649864\n",
      "Emma demurred .                                                                0.591435\n",
      "\" Are you well , my Emma ?\"                                                    0.591435\n",
      "At first it was downright dulness to Emma .                                    0.591435\n",
      "Emma was silenced .                                                            0.591435\n",
      "\" It is not now worth a regret ,\" said Emma .                                  0.576622\n",
      "Emma could not resist .                                                        0.569248\n",
      "\" Emma , my dear Emma \"                                                        0.535631\n",
      "This wretched note was the finale of Emma ' s breakfast .                      0.486627\n",
      "Emma was out of hearing .                                                      0.484317\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" Ah !      0.97956\n",
      "But ah !    0.97956\n",
      "\" Ah !      0.97956\n",
      "\" Ah !      0.97956\n",
      "\" Ah !      0.97956\n",
      "\" Ah !      0.97956\n",
      "But ah !    0.97956\n",
      "\" Ah !      0.97956\n",
      "\" Ah !      0.97956\n",
      "\" Ah !      0.97956\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.25, # drop words that occur in more than 1/4 the paragraphs\n",
    "                             min_df=3, # only use words that appear at least three times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changed:\n",
    "    \n",
    "max_df=0.25\n",
    "\n",
    "min_df=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1358\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "Tf_idf vector: {'minutes': array(0.71296533), 'present': array(0.70119928)}\n",
      "Percent variance captured by all components: 48.035550482034324\n",
      "Component 0:\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "Oh !      0.999255\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "\" Oh !    0.999255\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .                                                                                                                0.674362\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                 0.596223\n",
      "\" You get upon delicate subjects , Emma ,\" said Mrs . Weston smiling ; \" remember that I am here . Mr .                                                                     0.584202\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"    0.570551\n",
      "\" I do not admire it ,\" said Mr . Knightley .                                                                                                                               0.543288\n",
      "\" Mr . Weston will be almost as much relieved as myself ,\" said she .                                                                                                       0.541577\n",
      "Mr . Knightley might quarrel with her , but Emma could not quarrel with herself .                                                                                           0.539992\n",
      "\" Now ,\" said Emma , when they were fairly beyond the sweep gates , \" now Mr . Weston , do let me know what has happened .\"                                                 0.535007\n",
      "\" You are right , Mrs . Weston ,\" said Mr . Knightley warmly , \" Miss Fairfax is as capable as any of us of forming a just opinion of Mrs . Elton .                         0.532944\n",
      "\" Emma ,\" said Mr . Knightley presently , \" I have a piece of news for you .                                                                                                0.531993\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "CHAPTER I      0.998910\n",
      "CHAPTER V      0.998910\n",
      "CHAPTER X      0.998910\n",
      "CHAPTER I      0.998910\n",
      "CHAPTER X      0.998910\n",
      "CHAPTER V      0.998910\n",
      "CHAPTER X      0.998910\n",
      "CHAPTER V      0.998910\n",
      "CHAPTER I      0.998910\n",
      "CHAPTER XII    0.997998\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .    0.650405\n",
      "Emma demurred .                                                                0.588980\n",
      "\" Are you well , my Emma ?\"                                                    0.588980\n",
      "Emma was silenced .                                                            0.588980\n",
      "At first it was downright dulness to Emma .                                    0.588980\n",
      "Emma could not resist .                                                        0.564068\n",
      "\" It is not now worth a regret ,\" said Emma .                                  0.554608\n",
      "\" Emma , my dear Emma \"                                                        0.533770\n",
      "\" For shame , Emma !                                                           0.487748\n",
      "\" No great variety of faces for you ,\" said Emma .                             0.484003\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" Ah !      0.987568\n",
      "\" Ah !      0.987568\n",
      "But ah !    0.987568\n",
      "\" Ah !      0.987568\n",
      "\" Ah !      0.987568\n",
      "\" Ah !      0.987568\n",
      "\" Ah !      0.987568\n",
      "\" Ah !      0.987568\n",
      "\" Ah !      0.987568\n",
      "But ah !    0.987568\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             analyzer='char'\n",
    "                             smooth_idf=False #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changed:\n",
    "\n",
    "analyzer='char'\n",
    "\n",
    "smooth_idf=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
